{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO9grl9HzXXY8xUpk3ClEKU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["We look to train, validate and test a machine learning parameter by optimising its parameters using our data."],"metadata":{"id":"KyaSFBAxSduI"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"XKY5qcSPSSCX","executionInfo":{"status":"ok","timestamp":1702961769879,"user_tz":-780,"elapsed":3467,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)"]},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","\n","        # super() function is used to give access to methods and properties\n","        # of a parent or sibling class.\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"],"metadata":{"id":"w3uP-6FzTDFP","executionInfo":{"status":"ok","timestamp":1702961769880,"user_tz":-780,"elapsed":13,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Optimisation\n","Optimisation adjusts model parameters to reduce model error in each training step. Optimisation occurs in three steps:\n","1. optimizer.zero_grad(): reset gradients of model parameters as zero for each iteration (they add up by default).\n","2. Backpropagate prediction loss with a call to loss.backward() by computing the gradient of the loss function w.r.t. model parameters of interest.\n","3. Once we have gradients computed, we use \"optimizer.step()' to adjust parameters by gradients collected in backward pass."],"metadata":{"id":"aiklxMfNOQ2N"}},{"cell_type":"code","source":["# Training loop: iterate over training dataset and optimise the model\n","# parameters to perform its job best with our data.\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","  \"\"\"\n","\n","  \"\"\"\n","\n","  # Get the number of samples in dataset\n","  size = len(dataloader.dataset)\n","\n","  # Put model to training mode (important for batch normalisation and dropout layers)\n","  model.train()\n","\n","  # Divide entire dataset to smaller batches for parallel processing (improved computational efficiency)\n","  for batch, (X,y) in enumerate(dataloader):\n","\n","    # Compute model prediction and loss\n","    pred = model(X)\n","    loss = loss_fn(pred, y)\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # If complete batch, display model performance info\n","    if batch % 100 == 0:\n","      loss, current = loss.item(), (batch + 1) * len(X)\n","      print(\"Loss: {}, [{}/{}]\".format(loss, current, size))\n","\n","\n","# Validation/test loop: iterate over test dataset to check\n","# if model performance is improving.\n","def test_loop(dataloader, model, loss_fn):\n","  \"\"\"\n","\n","  \"\"\"\n","\n","  # Get the number of samples in dataset\n","  size = len(dataloader.dataset)\n","\n","  # Put model to evaluation mode (important for batch normalisation and dropout layers)\n","  model.eval()\n","  num_batches = len(dataloader)\n","  test_loss, correct = 0, 0\n","\n","  # We don't want to compute gradients of any model parameters during test mode,\n","  # so we set torch.no_grad(). We also want to reduce unnecessary gradient computations\n","  # and memory usage for tensors.\n","  with torch.no_grad():\n","    for X, y in dataloader:\n","      pred = model(X)\n","      test_loss += loss_fn(pred, y).item()\n","\n","      # WHAT IS THE COMPUTATION HERE??\n","      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","  test_loss /= num_batches\n","  correct /= size\n","  print(\"Test error: \\n Accuracy: {}%, Average loss: {} \\n\".format(\n","      100*correct, test_loss))"],"metadata":{"id":"crgOr3O6UmUQ","executionInfo":{"status":"ok","timestamp":1702961769880,"user_tz":-780,"elapsed":12,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def run_ml_workflow(epochs, train_dataloader, test_dataloader,\n","                    model, loss_fn, optimizer):\n","  \"\"\"\n","  \"\"\"\n","\n","  for idx in range(epochs):\n","    print(\"Epoch {}\\n ------------------------\".format(idx+1))\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","\n","  print(\"You have successfully trained and tested a machine learning model!!\")"],"metadata":{"id":"zTPvm1ABWGeX","executionInfo":{"status":"ok","timestamp":1702961769880,"user_tz":-780,"elapsed":12,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["We initialise the loss function and optimiser, then pass it to the training and testing loops. Note that you can change the number of epochs used as well.\n","\n","In this notebook, we are using the following:\n","- Loss function: Cross Entropy (**SEE FORMULATION**)\n","- Optimiser: Stochastic gradient descent (**REVIEW NOTES**)"],"metadata":{"id":"_rh3vrDjUT1L"}},{"cell_type":"code","source":["# Define hyperparameters of neural network prior to training (epochs, learning rate and batch size)\n","epochs = 20\n","learning_rate = 1e-03\n","batch_size = 128\n","\n","# Define loss function and optimiser to use for training\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# Run machine learning workflow\n","run_ml_workflow(epochs, train_dataloader, test_dataloader,\n","                model, loss_fn, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hr1Kfeb1UQuZ","executionInfo":{"status":"ok","timestamp":1702961987805,"user_tz":-780,"elapsed":217937,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"722ff9e2-5459-4474-9c5e-7a1313b8aaae"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n"," ------------------------\n","Loss: 2.296739339828491, [64/60000]\n","Loss: 2.288170099258423, [6464/60000]\n","Loss: 2.2718656063079834, [12864/60000]\n","Loss: 2.2706751823425293, [19264/60000]\n","Loss: 2.243852138519287, [25664/60000]\n","Loss: 2.217721700668335, [32064/60000]\n","Loss: 2.224879503250122, [38464/60000]\n","Loss: 2.191222906112671, [44864/60000]\n","Loss: 2.186361789703369, [51264/60000]\n","Loss: 2.1515936851501465, [57664/60000]\n","Test error: \n"," Accuracy: 39.300000000000004%, Average loss: 2.153448692552603 \n","\n","Epoch 2\n"," ------------------------\n","Loss: 2.162328004837036, [64/60000]\n","Loss: 2.1563799381256104, [6464/60000]\n","Loss: 2.100987434387207, [12864/60000]\n","Loss: 2.116605758666992, [19264/60000]\n","Loss: 2.0637600421905518, [25664/60000]\n","Loss: 2.0008583068847656, [32064/60000]\n","Loss: 2.031008243560791, [38464/60000]\n","Loss: 1.9532660245895386, [44864/60000]\n","Loss: 1.9515361785888672, [51264/60000]\n","Loss: 1.885230302810669, [57664/60000]\n","Test error: \n"," Accuracy: 55.510000000000005%, Average loss: 1.8851057040463588 \n","\n","Epoch 3\n"," ------------------------\n","Loss: 1.919417381286621, [64/60000]\n","Loss: 1.891226053237915, [6464/60000]\n","Loss: 1.7705037593841553, [12864/60000]\n","Loss: 1.8144683837890625, [19264/60000]\n","Loss: 1.7073478698730469, [25664/60000]\n","Loss: 1.6552417278289795, [32064/60000]\n","Loss: 1.679518222808838, [38464/60000]\n","Loss: 1.5844523906707764, [44864/60000]\n","Loss: 1.606152057647705, [51264/60000]\n","Loss: 1.5045256614685059, [57664/60000]\n","Test error: \n"," Accuracy: 59.91%, Average loss: 1.5213945997748406 \n","\n","Epoch 4\n"," ------------------------\n","Loss: 1.5908098220825195, [64/60000]\n","Loss: 1.554090976715088, [6464/60000]\n","Loss: 1.398097038269043, [12864/60000]\n","Loss: 1.4791475534439087, [19264/60000]\n","Loss: 1.3661993741989136, [25664/60000]\n","Loss: 1.356471300125122, [32064/60000]\n","Loss: 1.3736618757247925, [38464/60000]\n","Loss: 1.302714467048645, [44864/60000]\n","Loss: 1.3349329233169556, [51264/60000]\n","Loss: 1.2338792085647583, [57664/60000]\n","Test error: \n"," Accuracy: 62.62%, Average loss: 1.260708770934184 \n","\n","Epoch 5\n"," ------------------------\n","Loss: 1.341107964515686, [64/60000]\n","Loss: 1.3184220790863037, [6464/60000]\n","Loss: 1.1477221250534058, [12864/60000]\n","Loss: 1.261212944984436, [19264/60000]\n","Loss: 1.1454970836639404, [25664/60000]\n","Loss: 1.1628233194351196, [32064/60000]\n","Loss: 1.1849533319473267, [38464/60000]\n","Loss: 1.128039002418518, [44864/60000]\n","Loss: 1.1621516942977905, [51264/60000]\n","Loss: 1.0722731351852417, [57664/60000]\n","Test error: \n"," Accuracy: 64.45%, Average loss: 1.096317149271631 \n","\n","Epoch 6\n"," ------------------------\n","Loss: 1.1700754165649414, [64/60000]\n","Loss: 1.1680885553359985, [6464/60000]\n","Loss: 0.9812362194061279, [12864/60000]\n","Loss: 1.1225818395614624, [19264/60000]\n","Loss: 1.0044020414352417, [25664/60000]\n","Loss: 1.02921724319458, [32064/60000]\n","Loss: 1.064858317375183, [38464/60000]\n","Loss: 1.0140588283538818, [44864/60000]\n","Loss: 1.046644687652588, [51264/60000]\n","Loss: 0.9686738848686218, [57664/60000]\n","Test error: \n"," Accuracy: 65.97%, Average loss: 0.9876732879383548 \n","\n","Epoch 7\n"," ------------------------\n","Loss: 1.0473796129226685, [64/60000]\n","Loss: 1.0680478811264038, [6464/60000]\n","Loss: 0.8646451234817505, [12864/60000]\n","Loss: 1.0291192531585693, [19264/60000]\n","Loss: 0.9118831753730774, [25664/60000]\n","Loss: 0.9327214360237122, [32064/60000]\n","Loss: 0.9845109581947327, [38464/60000]\n","Loss: 0.9381310343742371, [44864/60000]\n","Loss: 0.9655261039733887, [51264/60000]\n","Loss: 0.8986687660217285, [57664/60000]\n","Test error: \n"," Accuracy: 67.4%, Average loss: 0.9127936583415718 \n","\n","Epoch 8\n"," ------------------------\n","Loss: 0.9562044143676758, [64/60000]\n","Loss: 0.9973053336143494, [6464/60000]\n","Loss: 0.7800520062446594, [12864/60000]\n","Loss: 0.962282657623291, [19264/60000]\n","Loss: 0.8486649394035339, [25664/60000]\n","Loss: 0.8613593578338623, [32064/60000]\n","Loss: 0.9276431798934937, [38464/60000]\n","Loss: 0.8866959810256958, [44864/60000]\n","Loss: 0.9065672755241394, [51264/60000]\n","Loss: 0.8484984636306763, [57664/60000]\n","Test error: \n"," Accuracy: 68.65%, Average loss: 0.8587775549311547 \n","\n","Epoch 9\n"," ------------------------\n","Loss: 0.8858202695846558, [64/60000]\n","Loss: 0.9438878893852234, [6464/60000]\n","Loss: 0.7164100408554077, [12864/60000]\n","Loss: 0.9122296571731567, [19264/60000]\n","Loss: 0.8034845590591431, [25664/60000]\n","Loss: 0.8074369430541992, [32064/60000]\n","Loss: 0.884720504283905, [38464/60000]\n","Loss: 0.8505978584289551, [44864/60000]\n","Loss: 0.8623375296592712, [51264/60000]\n","Loss: 0.8104903697967529, [57664/60000]\n","Test error: \n"," Accuracy: 70.04%, Average loss: 0.8179395510132905 \n","\n","Epoch 10\n"," ------------------------\n","Loss: 0.8294336795806885, [64/60000]\n","Loss: 0.9010533690452576, [6464/60000]\n","Loss: 0.667061984539032, [12864/60000]\n","Loss: 0.8730636239051819, [19264/60000]\n","Loss: 0.7697482109069824, [25664/60000]\n","Loss: 0.7659672498703003, [32064/60000]\n","Loss: 0.8502189517021179, [38464/60000]\n","Loss: 0.8239455819129944, [44864/60000]\n","Loss: 0.8279913663864136, [51264/60000]\n","Loss: 0.779991865158081, [57664/60000]\n","Test error: \n"," Accuracy: 71.38%, Average loss: 0.7856417619119025 \n","\n","Epoch 11\n"," ------------------------\n","Loss: 0.7831386923789978, [64/60000]\n","Loss: 0.864984393119812, [6464/60000]\n","Loss: 0.6275591850280762, [12864/60000]\n","Loss: 0.8416054844856262, [19264/60000]\n","Loss: 0.7431939244270325, [25664/60000]\n","Loss: 0.7336944937705994, [32064/60000]\n","Loss: 0.8208107948303223, [38464/60000]\n","Loss: 0.8031126856803894, [44864/60000]\n","Loss: 0.8005387187004089, [51264/60000]\n","Loss: 0.7545979619026184, [57664/60000]\n","Test error: \n"," Accuracy: 72.61999999999999%, Average loss: 0.7590383963220438 \n","\n","Epoch 12\n"," ------------------------\n","Loss: 0.7442658543586731, [64/60000]\n","Loss: 0.8335292935371399, [6464/60000]\n","Loss: 0.5951147675514221, [12864/60000]\n","Loss: 0.8158816695213318, [19264/60000]\n","Loss: 0.7213459610939026, [25664/60000]\n","Loss: 0.7081912159919739, [32064/60000]\n","Loss: 0.7948151230812073, [38464/60000]\n","Loss: 0.785834014415741, [44864/60000]\n","Loss: 0.7779898047447205, [51264/60000]\n","Loss: 0.7327940464019775, [57664/60000]\n","Test error: \n"," Accuracy: 73.74000000000001%, Average loss: 0.7362939797009632 \n","\n","Epoch 13\n"," ------------------------\n","Loss: 0.7109118700027466, [64/60000]\n","Loss: 0.8054171204566956, [6464/60000]\n","Loss: 0.5677298903465271, [12864/60000]\n","Loss: 0.7942554354667664, [19264/60000]\n","Loss: 0.70284104347229, [25664/60000]\n","Loss: 0.6874548196792603, [32064/60000]\n","Loss: 0.7711912393569946, [38464/60000]\n","Loss: 0.770793080329895, [44864/60000]\n","Loss: 0.7587876319885254, [51264/60000]\n","Loss: 0.7134976387023926, [57664/60000]\n","Test error: \n"," Accuracy: 74.37%, Average loss: 0.7162613620044319 \n","\n","Epoch 14\n"," ------------------------\n","Loss: 0.6817744374275208, [64/60000]\n","Loss: 0.7798123955726624, [6464/60000]\n","Loss: 0.5441194772720337, [12864/60000]\n","Loss: 0.7755624055862427, [19264/60000]\n","Loss: 0.6868373155593872, [25664/60000]\n","Loss: 0.6703191995620728, [32064/60000]\n","Loss: 0.7494320869445801, [38464/60000]\n","Loss: 0.7574094533920288, [44864/60000]\n","Loss: 0.7422842383384705, [51264/60000]\n","Loss: 0.6961888670921326, [57664/60000]\n","Test error: \n"," Accuracy: 75.11%, Average loss: 0.6982425344977409 \n","\n","Epoch 15\n"," ------------------------\n","Loss: 0.6561099886894226, [64/60000]\n","Loss: 0.7562476992607117, [6464/60000]\n","Loss: 0.5233891010284424, [12864/60000]\n","Loss: 0.759107232093811, [19264/60000]\n","Loss: 0.6728612780570984, [25664/60000]\n","Loss: 0.6557950377464294, [32064/60000]\n","Loss: 0.7292289733886719, [38464/60000]\n","Loss: 0.7452527284622192, [44864/60000]\n","Loss: 0.727859616279602, [51264/60000]\n","Loss: 0.6804154515266418, [57664/60000]\n","Test error: \n"," Accuracy: 75.89%, Average loss: 0.6818199425366274 \n","\n","Epoch 16\n"," ------------------------\n","Loss: 0.6332529187202454, [64/60000]\n","Loss: 0.7345805764198303, [6464/60000]\n","Loss: 0.5049963593482971, [12864/60000]\n","Loss: 0.7443894147872925, [19264/60000]\n","Loss: 0.6603769063949585, [25664/60000]\n","Loss: 0.6433203220367432, [32064/60000]\n","Loss: 0.7103845477104187, [38464/60000]\n","Loss: 0.7342344522476196, [44864/60000]\n","Loss: 0.7153612971305847, [51264/60000]\n","Loss: 0.6659514904022217, [57664/60000]\n","Test error: \n"," Accuracy: 76.73%, Average loss: 0.6667918374963627 \n","\n","Epoch 17\n"," ------------------------\n","Loss: 0.6128559112548828, [64/60000]\n","Loss: 0.7145894169807434, [6464/60000]\n","Loss: 0.4886009395122528, [12864/60000]\n","Loss: 0.7310795783996582, [19264/60000]\n","Loss: 0.6493237018585205, [25664/60000]\n","Loss: 0.6324312686920166, [32064/60000]\n","Loss: 0.6927399635314941, [38464/60000]\n","Loss: 0.7244488000869751, [44864/60000]\n","Loss: 0.7044540643692017, [51264/60000]\n","Loss: 0.6524903178215027, [57664/60000]\n","Test error: \n"," Accuracy: 77.19%, Average loss: 0.6529986721694849 \n","\n","Epoch 18\n"," ------------------------\n","Loss: 0.5945405960083008, [64/60000]\n","Loss: 0.6961937546730042, [6464/60000]\n","Loss: 0.4740290641784668, [12864/60000]\n","Loss: 0.7189491987228394, [19264/60000]\n","Loss: 0.6393047571182251, [25664/60000]\n","Loss: 0.6228985786437988, [32064/60000]\n","Loss: 0.6762773394584656, [38464/60000]\n","Loss: 0.7158645391464233, [44864/60000]\n","Loss: 0.6950851082801819, [51264/60000]\n","Loss: 0.6400227546691895, [57664/60000]\n","Test error: \n"," Accuracy: 77.64999999999999%, Average loss: 0.6403320855016161 \n","\n","Epoch 19\n"," ------------------------\n","Loss: 0.577905535697937, [64/60000]\n","Loss: 0.6792765259742737, [6464/60000]\n","Loss: 0.46092724800109863, [12864/60000]\n","Loss: 0.7077565789222717, [19264/60000]\n","Loss: 0.6303158402442932, [25664/60000]\n","Loss: 0.6144806742668152, [32064/60000]\n","Loss: 0.6609440445899963, [38464/60000]\n","Loss: 0.7083479166030884, [44864/60000]\n","Loss: 0.6869741678237915, [51264/60000]\n","Loss: 0.6284171342849731, [57664/60000]\n","Test error: \n"," Accuracy: 78.13%, Average loss: 0.6286763989242019 \n","\n","Epoch 20\n"," ------------------------\n","Loss: 0.562729001045227, [64/60000]\n","Loss: 0.663851261138916, [6464/60000]\n","Loss: 0.4490813612937927, [12864/60000]\n","Loss: 0.6973060369491577, [19264/60000]\n","Loss: 0.6220707893371582, [25664/60000]\n","Loss: 0.6069218516349792, [32064/60000]\n","Loss: 0.6465528011322021, [38464/60000]\n","Loss: 0.7019123435020447, [44864/60000]\n","Loss: 0.6800214052200317, [51264/60000]\n","Loss: 0.6175153851509094, [57664/60000]\n","Test error: \n"," Accuracy: 78.56%, Average loss: 0.6179406828941054 \n","\n","You have successfully trained and tested a machine learning model!!\n"]}]}]}