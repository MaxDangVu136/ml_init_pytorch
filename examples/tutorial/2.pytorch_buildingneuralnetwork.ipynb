{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOgzI5T148ZKTE+2IF3wYO4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["All the building blocks we need to build a neural network in PyTorch can be found using the torch.nn module.\n","\n","This example builds a neural network to classify images in the FashionMNIST dataset.\n"],"metadata":{"id":"x9pYuWayMo18"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"j8fkvUCjMV6T","executionInfo":{"status":"ok","timestamp":1702808194499,"user_tz":-780,"elapsed":523,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"code","source":["# Get a GPU containing device for training\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","\n","print(\"Using {} device\".format(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehiLwiZVNNgm","executionInfo":{"status":"ok","timestamp":1702808195027,"user_tz":-780,"elapsed":27,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"fe8197ce-8750-4223-a344-ab7e7baaa206"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}]},{"cell_type":"markdown","source":["# Define a class for building a neural network"],"metadata":{"id":"Bz6xL816ONcX"}},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","\n","  def __init__(self):\n","    \"\"\"\n","\n","    \"\"\"\n","    super().__init__()\n","    self.flatten = nn.Flatten()\n","    self.linear_relu_stack = nn.Sequential(\n","        nn.Linear(28*28, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        nn.Linear(512,10),\n","    )\n","\n","  def forward(self, x):\n","    \"\"\"\n","    \"\"\"\n","    x = self.flatten(x)\n","    logits = self.linear_relu_stack(x)\n","    return logits"],"metadata":{"id":"siFhx1f8NqAU","executionInfo":{"status":"ok","timestamp":1702808195027,"user_tz":-780,"elapsed":25,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model = NeuralNetwork().to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qUf51KB1PFqM","executionInfo":{"status":"ok","timestamp":1702808195027,"user_tz":-780,"elapsed":24,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"f0742aac-6b06-4eaf-c4df-d81ea69df0d2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["#   Create a minibatch of 1 image of size 28 x 28 pixels\n","X = torch.rand(1, 28, 28, device=device)\n","\n","#   Use model by passing it input data to execute model's forward function\n","logits = model(X)\n","\n","#   Compute output using softmax activation function\n","pred_probab = nn.Softmax(dim=1)(logits)\n","\n","#   Predicted output from model\n","y_pred = pred_probab.argmax(1)\n","\n","print(\"Predicted class: {}\".format(y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiuJ32TjPU3J","executionInfo":{"status":"ok","timestamp":1702808195027,"user_tz":-780,"elapsed":23,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"ac3d147a-5307-4c12-bc66-126e0ad043dc"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: tensor([1], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## Understanding model layers"],"metadata":{"id":"RxoVJIF8Qz4M"}},{"cell_type":"code","source":["#   Image size\n","img_size = (28, 28)\n","img_size[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BelbODT5Rei-","executionInfo":{"status":"ok","timestamp":1702808195028,"user_tz":-780,"elapsed":22,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"0a2ccb9e-2c90-44db-dc57-9319e17adfa1"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#   Initialise the size of the input image\n","input_img = torch.rand(3, img_size[0], img_size[1])\n","print(input_img.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kIulqCbQnsW","executionInfo":{"status":"ok","timestamp":1702808195028,"user_tz":-780,"elapsed":20,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"81bbc28c-0b13-488c-8a31-659b21933db0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}]},{"cell_type":"code","source":["#   Convert a 28 x 28 image into a contiguous array (so looks like 1D?) of 784 pixel values\n","flatten = nn.Flatten()\n","flat_img = flatten(input_img)\n","print(flat_img.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phdVR43lQ7mY","executionInfo":{"status":"ok","timestamp":1702808195028,"user_tz":-780,"elapsed":19,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"1a3d5039-c86e-412d-c2b3-c8200dab6e0f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}]},{"cell_type":"markdown","source":["### nn.Linear"],"metadata":{"id":"ti9DxMASSFBh"}},{"cell_type":"code","source":["#   Applies a linear transformation on the input image using weights and biases\n","layer1 = nn.Linear(in_features=img_size[0]*img_size[1], out_features=20)\n","hidden1 = layer1(flat_img)\n","print(hidden1.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uywPCsNRTrj","executionInfo":{"status":"ok","timestamp":1702808195028,"user_tz":-780,"elapsed":18,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"892ab518-cf93-4a74-b7ea-d80520380b40"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}]},{"cell_type":"markdown","source":["### nn.ReLU"],"metadata":{"id":"Jw8pS4l7SIHz"}},{"cell_type":"code","source":["##  ReLU is a non-linear activation which introduces nonlinearity between\n","##  linear layers to create complex mappings between model inputs and output\n","##  (and therefore learn a wide variety of phenomena)\n","\n","##  ReLU works as follows: y = x (if x > 0), else y = 0.\n","print(\"Before ReLU: {}\".format(hidden1))\n","hidden1 = nn.ReLU()(hidden1)\n","print(\"After ReLU: {}\".format(hidden1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYcSYzpaRw_9","executionInfo":{"status":"ok","timestamp":1702808422227,"user_tz":-780,"elapsed":508,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"3bf76d98-7828-462b-c9c3-1196d55032ea"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[ 2.6399e-01, -1.7711e-01, -1.5105e-01, -2.3161e-01, -3.0043e-01,\n","          7.5082e-02, -6.6009e-03, -3.2847e-02, -4.5309e-02, -3.0627e-01,\n","         -9.4765e-02, -8.4134e-01, -3.2944e-01, -2.4268e-01,  2.3555e-01,\n","         -2.8056e-01, -2.2567e-02, -5.4899e-02, -3.4474e-01, -4.0673e-01],\n","        [ 1.5378e-01, -2.8663e-01,  2.9110e-03, -1.3168e-01, -1.7538e-01,\n","          1.3143e-01,  7.3966e-03,  4.1303e-01,  2.2569e-01, -4.3359e-01,\n","         -1.3441e-01, -5.0002e-01, -2.0246e-01, -4.0853e-01,  3.5603e-01,\n","         -2.3559e-01, -1.8593e-01,  2.9791e-02,  2.8838e-01, -2.8012e-01],\n","        [ 3.5205e-01, -4.4486e-01, -4.2821e-01, -1.9053e-02, -2.3811e-01,\n","          1.9506e-01, -3.9960e-01,  3.7430e-01, -2.1434e-02, -1.9904e-01,\n","          2.3641e-01, -7.8826e-01, -1.6622e-01, -4.5980e-01,  1.2718e-01,\n","         -1.5431e-01, -3.5686e-01, -6.7784e-05,  4.8975e-02, -5.5473e-01]],\n","       grad_fn=<AddmmBackward0>)\n","After ReLU: tensor([[0.2640, 0.0000, 0.0000, 0.0000, 0.0000, 0.0751, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2355, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.0000],\n","        [0.1538, 0.0000, 0.0029, 0.0000, 0.0000, 0.1314, 0.0074, 0.4130, 0.2257,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3560, 0.0000, 0.0000, 0.0298,\n","         0.2884, 0.0000],\n","        [0.3521, 0.0000, 0.0000, 0.0000, 0.0000, 0.1951, 0.0000, 0.3743, 0.0000,\n","         0.0000, 0.2364, 0.0000, 0.0000, 0.0000, 0.1272, 0.0000, 0.0000, 0.0000,\n","         0.0490, 0.0000]], grad_fn=<ReluBackward0>)\n"]}]},{"cell_type":"markdown","source":["### nn.Sequential"],"metadata":{"id":"8tX53R_vS1hH"}},{"cell_type":"code","source":["##  nn.Sequential is ordered container of modules, which determines the\n","##  route of data being passed through the network.\n","\n","seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","\n","input_img = torch.rand(3, 28, 28)\n","logits = seq_modules(input_img)"],"metadata":{"id":"WvquJ65QSzQQ","executionInfo":{"status":"ok","timestamp":1702808583276,"user_tz":-780,"elapsed":16,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["##  nn.Softmax scales the outputs of the last linear layer of NN to values from\n","##  0 to 1, as these values are logits (between -inf and inf).\n","\n","softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"],"metadata":{"id":"jgdGrhEnTP8V","executionInfo":{"status":"ok","timestamp":1702809119372,"user_tz":-780,"elapsed":503,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(\"Model structure: {}\".format(model))\n","\n","for name, param in model.named_parameters():\n","  print(\"Layer: {} | Size: {} | Values: {} \\n\".\n","        format(name, param.size(), param[:2]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGsshw-uVXnY","executionInfo":{"status":"ok","timestamp":1702809344333,"user_tz":-780,"elapsed":23,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"8b2c6137-9735-437f-9253-653e420323f9"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0315,  0.0254,  0.0256,  ..., -0.0116, -0.0069, -0.0164],\n","        [-0.0226, -0.0326,  0.0136,  ..., -0.0335, -0.0276, -0.0195]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([0.0341, 0.0261], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[-0.0070, -0.0411,  0.0338,  ..., -0.0288,  0.0358,  0.0399],\n","        [-0.0325, -0.0072,  0.0408,  ..., -0.0227, -0.0253, -0.0226]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0147,  0.0185], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[-0.0346,  0.0417,  0.0128,  ...,  0.0303,  0.0254,  0.0100],\n","        [ 0.0092,  0.0016,  0.0326,  ...,  0.0221,  0.0260, -0.0310]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0438,  0.0289], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n"]}]}]}