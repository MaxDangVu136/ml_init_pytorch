{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO0LVeZTmnHk0Ir3Bya7A3G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Automatic differentiation\n","torch.autograd is a built-in differentiation engine that:\n","- automatically computes the gradient of any computational graph.\n","- keeps a record of data (tensors) and all executed options (along with resulting new tensors).\n","\n","This is useful for neural network training, specifically the back propagation algorithm where the model weights (or parameter) are adjusted based on the gradient of the loss function w.r.t. given model parameter."],"metadata":{"id":"6BrVyR9CC7Kl"}},{"cell_type":"markdown","source":["## NEED TO UNDERSTAND BACK PROPRAGATION ALGORITHM AND EACH EQUATION (ENGSCI 712)"],"metadata":{"id":"QgiHbsylESkX"}},{"cell_type":"code","source":["import torch\n","\n","# When a tensor is first created - it becomes a leaf node.\n","# All inputs and weights of neural network are leaf nodes of computational graph.\n","# When any operation is performed on a tensor - it is not a leaf node anymore.\n","x = torch.ones(5)\n","y = torch.zeros(3)\n","w = torch.randn(5, 3, requires_grad=True)  #  nn parameters that we want to optimise.\n","b = torch.randn(3, requires_grad=True) #  nn parameters that we want to optimise.\n","z = torch.matmul(x, w) + b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"],"metadata":{"id":"C08-vNnHE2rW","executionInfo":{"status":"ok","timestamp":1702889190450,"user_tz":-780,"elapsed":3918,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(\"Gradient function for z = {}\".format(z.grad_fn))\n","print(\"Gradient function for loss = {}\".format(loss.grad_fn))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tsi6fOaDFRmC","executionInfo":{"status":"ok","timestamp":1702889408516,"user_tz":-780,"elapsed":425,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"3f3db542-04f8-4bf8-91ed-953ac81cbc98"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7be8d3a41ae0>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7be8213c7ee0>\n"]}]},{"cell_type":"markdown","source":["Remember that we set \"requires_grad=True\" for a tensor when we want to optimise the parameter for that tensor. We can do this after creating the tensor, using \"x.requires_grad_(True)\"."],"metadata":{"id":"0u9hQIb6KRDh"}},{"cell_type":"markdown","source":["## Computing gradients"],"metadata":{"id":"F9TuphdWH7KS"}},{"cell_type":"markdown","source":["We want to compute the derivatives of our loss function w.r.t. the neural network parameters that we want to optimise."],"metadata":{"id":"jyBIPnqXH_BE"}},{"cell_type":"code","source":["loss.backward()\n","print(\"dLoss/dw:\", w.grad)\n","print(\"dLoss/db:\", b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuIZ_ZrlH9Q6","executionInfo":{"status":"ok","timestamp":1702889817021,"user_tz":-780,"elapsed":18,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"e646a161-7f7e-43d5-8256-bbfcfcea0934"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["dLoss/dw: tensor([[0.3303, 0.2642, 0.3327],\n","        [0.3303, 0.2642, 0.3327],\n","        [0.3303, 0.2642, 0.3327],\n","        [0.3303, 0.2642, 0.3327],\n","        [0.3303, 0.2642, 0.3327]])\n","dLoss/db: tensor([0.3303, 0.2642, 0.3327])\n"]}]},{"cell_type":"markdown","source":["After each \".backward()\" call, autograd is recreated from scratch and populates a new graph. What also happens after \".backward()\" is called:\n","- computes gradients from each \".grad_fn\"\n","- accumulates them in that tensor's \".grad\" attribute\n","- using chain rule, propagates all the way to leaf tensors."],"metadata":{"id":"PQMV8_2-QPbU"}},{"cell_type":"markdown","source":["## Disabling gradient tracking"],"metadata":{"id":"UXX_nKpDMCLM"}},{"cell_type":"markdown","source":["Why do we want to disable gradient tracking?\n","- Mark parameters in neural network as frozen parameters\n","- Speed up computations when performing forward pass, tracking gradients is less efficient."],"metadata":{"id":"tWRXiInbNIt3"}},{"cell_type":"code","source":["z = torch.matmul(x, w) + b\n","print(z.requires_grad)\n","\n","# How we can stop tracking computations so that we only perform forward\n","# computations through network.\n","with torch.no_grad():\n","  z = torch.matmul(x, w) + b\n","print(z.requires_grad)\n","\n","# We can do the equivalent of torch.no_grad() using \"parameter\".detach().\n","z = torch.matmul(x, w) + b\n","z_det = z.detach()\n","print(z_det.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIIVdg58MAy-","executionInfo":{"status":"ok","timestamp":1702890857099,"user_tz":-780,"elapsed":13,"user":{"displayName":"Max Dang Vu","userId":"02858938018569343923"}},"outputId":"96dbd55e-918a-4253-c04f-a2b87902ecde"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n","False\n"]}]}]}